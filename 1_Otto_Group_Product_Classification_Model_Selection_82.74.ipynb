{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import shapiro, normaltest\n",
    "from tpot import TPOTClassifier\n",
    "from hpsklearn import HyperoptEstimator, pca, min_max_scaler, standard_scaler\n",
    "from hpsklearn import xgboost_classification, random_forest, ada_boost, gradient_boosting, extra_trees\n",
    "from hpsklearn import svc, svc_linear, svc_rbf, svc_poly, svc_sigmoid, liblinear_svc\n",
    "from hpsklearn import any_classifier\n",
    "from hpsklearn import any_preprocessing\n",
    "from hyperopt import tpe, hp\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report,confusion_matrix,plot_confusion_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, RFECV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/otto_group_Product_train.csv')\n",
    "#df_test = pd.read_csv('Data/otto_group_Product_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61878 entries, 0 to 61877\n",
      "Data columns (total 95 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       61878 non-null  int64 \n",
      " 1   feat_1   61878 non-null  int64 \n",
      " 2   feat_2   61878 non-null  int64 \n",
      " 3   feat_3   61878 non-null  int64 \n",
      " 4   feat_4   61878 non-null  int64 \n",
      " 5   feat_5   61878 non-null  int64 \n",
      " 6   feat_6   61878 non-null  int64 \n",
      " 7   feat_7   61878 non-null  int64 \n",
      " 8   feat_8   61878 non-null  int64 \n",
      " 9   feat_9   61878 non-null  int64 \n",
      " 10  feat_10  61878 non-null  int64 \n",
      " 11  feat_11  61878 non-null  int64 \n",
      " 12  feat_12  61878 non-null  int64 \n",
      " 13  feat_13  61878 non-null  int64 \n",
      " 14  feat_14  61878 non-null  int64 \n",
      " 15  feat_15  61878 non-null  int64 \n",
      " 16  feat_16  61878 non-null  int64 \n",
      " 17  feat_17  61878 non-null  int64 \n",
      " 18  feat_18  61878 non-null  int64 \n",
      " 19  feat_19  61878 non-null  int64 \n",
      " 20  feat_20  61878 non-null  int64 \n",
      " 21  feat_21  61878 non-null  int64 \n",
      " 22  feat_22  61878 non-null  int64 \n",
      " 23  feat_23  61878 non-null  int64 \n",
      " 24  feat_24  61878 non-null  int64 \n",
      " 25  feat_25  61878 non-null  int64 \n",
      " 26  feat_26  61878 non-null  int64 \n",
      " 27  feat_27  61878 non-null  int64 \n",
      " 28  feat_28  61878 non-null  int64 \n",
      " 29  feat_29  61878 non-null  int64 \n",
      " 30  feat_30  61878 non-null  int64 \n",
      " 31  feat_31  61878 non-null  int64 \n",
      " 32  feat_32  61878 non-null  int64 \n",
      " 33  feat_33  61878 non-null  int64 \n",
      " 34  feat_34  61878 non-null  int64 \n",
      " 35  feat_35  61878 non-null  int64 \n",
      " 36  feat_36  61878 non-null  int64 \n",
      " 37  feat_37  61878 non-null  int64 \n",
      " 38  feat_38  61878 non-null  int64 \n",
      " 39  feat_39  61878 non-null  int64 \n",
      " 40  feat_40  61878 non-null  int64 \n",
      " 41  feat_41  61878 non-null  int64 \n",
      " 42  feat_42  61878 non-null  int64 \n",
      " 43  feat_43  61878 non-null  int64 \n",
      " 44  feat_44  61878 non-null  int64 \n",
      " 45  feat_45  61878 non-null  int64 \n",
      " 46  feat_46  61878 non-null  int64 \n",
      " 47  feat_47  61878 non-null  int64 \n",
      " 48  feat_48  61878 non-null  int64 \n",
      " 49  feat_49  61878 non-null  int64 \n",
      " 50  feat_50  61878 non-null  int64 \n",
      " 51  feat_51  61878 non-null  int64 \n",
      " 52  feat_52  61878 non-null  int64 \n",
      " 53  feat_53  61878 non-null  int64 \n",
      " 54  feat_54  61878 non-null  int64 \n",
      " 55  feat_55  61878 non-null  int64 \n",
      " 56  feat_56  61878 non-null  int64 \n",
      " 57  feat_57  61878 non-null  int64 \n",
      " 58  feat_58  61878 non-null  int64 \n",
      " 59  feat_59  61878 non-null  int64 \n",
      " 60  feat_60  61878 non-null  int64 \n",
      " 61  feat_61  61878 non-null  int64 \n",
      " 62  feat_62  61878 non-null  int64 \n",
      " 63  feat_63  61878 non-null  int64 \n",
      " 64  feat_64  61878 non-null  int64 \n",
      " 65  feat_65  61878 non-null  int64 \n",
      " 66  feat_66  61878 non-null  int64 \n",
      " 67  feat_67  61878 non-null  int64 \n",
      " 68  feat_68  61878 non-null  int64 \n",
      " 69  feat_69  61878 non-null  int64 \n",
      " 70  feat_70  61878 non-null  int64 \n",
      " 71  feat_71  61878 non-null  int64 \n",
      " 72  feat_72  61878 non-null  int64 \n",
      " 73  feat_73  61878 non-null  int64 \n",
      " 74  feat_74  61878 non-null  int64 \n",
      " 75  feat_75  61878 non-null  int64 \n",
      " 76  feat_76  61878 non-null  int64 \n",
      " 77  feat_77  61878 non-null  int64 \n",
      " 78  feat_78  61878 non-null  int64 \n",
      " 79  feat_79  61878 non-null  int64 \n",
      " 80  feat_80  61878 non-null  int64 \n",
      " 81  feat_81  61878 non-null  int64 \n",
      " 82  feat_82  61878 non-null  int64 \n",
      " 83  feat_83  61878 non-null  int64 \n",
      " 84  feat_84  61878 non-null  int64 \n",
      " 85  feat_85  61878 non-null  int64 \n",
      " 86  feat_86  61878 non-null  int64 \n",
      " 87  feat_87  61878 non-null  int64 \n",
      " 88  feat_88  61878 non-null  int64 \n",
      " 89  feat_89  61878 non-null  int64 \n",
      " 90  feat_90  61878 non-null  int64 \n",
      " 91  feat_91  61878 non-null  int64 \n",
      " 92  feat_92  61878 non-null  int64 \n",
      " 93  feat_93  61878 non-null  int64 \n",
      " 94  target   61878 non-null  object\n",
      "dtypes: int64(94), object(1)\n",
      "memory usage: 44.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 95)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "feat_1     0\n",
       "feat_2     0\n",
       "feat_3     0\n",
       "feat_4     0\n",
       "          ..\n",
       "feat_90    0\n",
       "feat_91    0\n",
       "feat_92    0\n",
       "feat_93    0\n",
       "target     0\n",
       "Length: 95, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns = {}\n",
    "\n",
    "all_columns = df_train.isnull().sum().sort_values(ascending=False)\n",
    "for item in all_columns.index:\n",
    "    if all_columns[item] > 0:\n",
    "        null_columns[item] = 100* all_columns[item]/len(df)\n",
    "        \n",
    "null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# calculate duplicates\n",
    "dups = df_train.duplicated()\n",
    "#print(dups)\n",
    "# report if there are any duplicates\n",
    "print(dups.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target         9\n",
       "feat_6         9\n",
       "feat_5        15\n",
       "feat_21       15\n",
       "feat_37       18\n",
       "           ...  \n",
       "feat_90       91\n",
       "feat_74      101\n",
       "feat_19      105\n",
       "feat_73      115\n",
       "id         61878\n",
       "Length: 95, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize the number of unique values in each column\n",
    "df_train.nunique().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Class_1', 'Class_1', 'Class_1', ..., 'Class_9', 'Class_9',\n",
       "       'Class_9'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class_2    16122\n",
       "Class_6    14135\n",
       "Class_8     8464\n",
       "Class_3     8004\n",
       "Class_9     4955\n",
       "Class_7     2839\n",
       "Class_5     2739\n",
       "Class_4     2691\n",
       "Class_1     1929\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_train.drop(['id', 'target'],axis=1)\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43314, 93)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18564, 93)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Find Best Algorithm with Best Params Using HyperoptEstimator AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preproc = hp.choice('myprepros_name', \n",
    "                    [\n",
    "                        [min_max_scaler('myprepros_name.only_norm')],\n",
    "                        [standard_scaler('myprepros_name.only_std_scaler')],\n",
    "                        [pca('myprepros_name.only_pca')],\n",
    "                        [pca('myprepros_name.pca'), min_max_scaler('myprepros_name.norm')],\n",
    "                        [min_max_scaler('myprepros_name.first_norm'), standard_scaler('myprepros_name.second_std_scaler')],\n",
    "                        []\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = hp.choice( 'model_name',\n",
    "            [ random_forest ('model_name.random_forest'),\n",
    "            ada_boost ('model_name.ada_boost'),\n",
    "            gradient_boosting ('model_name.gradient_boosting'),\n",
    "            xgboost_classification ('model_name.xgboost_classification'),\n",
    "            extra_trees('model_name.extra_trees'),\n",
    "            svc_linear('model_name.svc_linear'),\n",
    "            svc_rbf('model_name.svc_rbf'),\n",
    "            svc('model_name.svc')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#preprocessing=[min_max_scaler('norm_scaler'), pca('my_pca') ]\n",
    "#classifier=xgboost_classification('xgboost')\n",
    "#preprocessing=any_preprocessing('pre')\n",
    "#preprocessing = preproc\n",
    "#classifier=any_classifier('cla')\n",
    "#classifier=clf\n",
    "\n",
    "#01. 100 times with classifier=any_classifier('cla') & preprocessing=any_preprocessing('pre')\n",
    "\n",
    "#02. 100 times with classifier=clf & preprocessing = preproc\n",
    "\n",
    "#03. 100 times with classifier=xgboost_classification('xgboost') & preprocessing = preproc\n",
    "\n",
    "#04. 100 times with classifier=xgboost_classification('xgboost') & preprocessing = any_preprocessing('pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Started at: 2020-11-04 10:56:12.812090\n",
      "100%|██████████████████████████████████████████████████████████████| 1/1 [03:32<00:00, 212.86s/trial, best loss: 0.2234566342633506]\n",
      "100%|███████████████████████████████████████████████████████████████| 2/2 [01:29<00:00, 44.73s/trial, best loss: 0.2234566342633506]\n",
      "100%|███████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.60s/trial, best loss: 0.2234566342633506]\n",
      "100%|███████████████████████████████████████████████████████████████| 4/4 [04:32<00:00, 68.23s/trial, best loss: 0.2234566342633506]\n",
      "100%|███████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.37s/trial, best loss: 0.2234566342633506]\n",
      "100%|█████████████████████████████████████████████████████████████| 6/6 [27:00<00:00, 270.03s/trial, best loss: 0.20388839792955438]\n",
      "100%|██████████████████████████████████████████████████████████████| 7/7 [09:02<00:00, 77.45s/trial, best loss: 0.20388839792955438]\n",
      "100%|██████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.18s/trial, best loss: 0.20388839792955438]\n",
      "100%|██████████████████████████████████████████████████████████████| 9/9 [00:42<00:00,  4.69s/trial, best loss: 0.20388839792955438]\n",
      "100%|████████████████████████████████████████████████████████████| 10/10 [00:41<00:00,  4.16s/trial, best loss: 0.20388839792955438]\n",
      "100%|████████████████████████████████████████████████████████████| 11/11 [08:29<00:00, 46.29s/trial, best loss: 0.20388839792955438]\n",
      "100%|████████████████████████████████████████████████████████████| 12/12 [00:28<00:00,  2.41s/trial, best loss: 0.20388839792955438]\n",
      "100%|████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.57trial/s, best loss: 0.20388839792955438]\n",
      "100%|███████████████████████████████████████████████████████████| 14/14 [27:25<00:00, 117.53s/trial, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 15/15 [00:08<00:00,  1.77trial/s, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 16/16 [01:13<00:00,  4.61s/trial, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 17/17 [01:15<00:00,  4.42s/trial, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 18/18 [01:43<00:00,  5.73s/trial, best loss: 0.18116399444514586]\n",
      "100%|███████████████████████████████████████████████████████████| 19/19 [35:34<00:00, 112.36s/trial, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.78trial/s, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 21/21 [00:10<00:00,  2.08trial/s, best loss: 0.18116399444514586]\n",
      "100%|████████████████████████████████████████████████████████████| 22/22 [48:33<00:00, 132.42s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 23/23 [02:11<00:00,  5.71s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 24/24 [01:20<00:00,  3.36s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 25/25 [17:21<00:00, 41.65s/trial, best loss: 0.1785128140386315]\n",
      "100%|████████████████████████████████████████████████████████████| 26/26 [52:54<00:00, 122.08s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 27/27 [41:57<00:00, 93.24s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 28/28 [15:48<00:00, 33.86s/trial, best loss: 0.1785128140386315]\n",
      "100%|██████████████████████████████████████████████████████████| 29/29 [1:08:48<00:00, 142.37s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 30/30 [00:04<00:00,  6.72trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 31/31 [00:04<00:00,  6.42trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.94trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 33/33 [40:19<00:00, 73.31s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 34/34 [00:08<00:00,  3.86trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 35/35 [05:55<00:00, 10.16s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 36/36 [00:35<00:00,  1.01trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 37/37 [30:43<00:00, 49.83s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 38/38 [30:12<00:00, 47.70s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 39/39 [04:04<00:00,  6.28s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 40/40 [46:56<00:00, 70.42s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 41/41 [00:09<00:00,  4.20trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 42/42 [14:43<00:00, 21.04s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 43/43 [01:09<00:00,  1.61s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 44/44 [01:10<00:00,  1.59s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 45/45 [00:12<00:00,  3.52trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 46/46 [41:19<00:00, 53.90s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 47/47 [03:50<00:00,  4.91s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.31trial/s, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 49/49 [01:11<00:00,  1.46s/trial, best loss: 0.1785128140386315]\n",
      "100%|█████████████████████████████████████████████████████████████| 50/50 [03:36<00:00,  4.32s/trial, best loss: 0.1785128140386315]\n",
      "[20:56:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Execution time :  11:02:40.515554\n",
      "Job Ended at: 2020-11-04 21:58:53.327644\n"
     ]
    }
   ],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "accuracy = 0\n",
    "best_model = None\n",
    "#for i in range(100):\n",
    "    #print (i)\n",
    "model = HyperoptEstimator(  classifier= any_classifier('cla'), \n",
    "                                preprocessing= any_preprocessing('pre'), \n",
    "                                algo=tpe.suggest, \n",
    "                                max_evals=50, \n",
    "                                trial_timeout=5000)\n",
    "    # perform the search\n",
    "model.fit(X_train, y_train)\n",
    "acc = model.score(X_test, y_test)\n",
    "    #if acc > accuracy:\n",
    "        #accuracy = acc\n",
    "        #best_model = model\n",
    "        #print (accuracy)\n",
    "\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.827\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.827\n",
      "{'learner': XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "              colsample_bylevel=0.9783479287357171, colsample_bynode=1,\n",
      "              colsample_bytree=0.8174662246984075, gamma=0.6596707384106874,\n",
      "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.025933472296031868, max_delta_step=0, max_depth=9,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=3600, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0,\n",
      "              reg_alpha=0.05907593535724862, reg_lambda=1.0398714478065672,\n",
      "              scale_pos_weight=1, seed=0, subsample=0.5081679750943192,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None), 'preprocs': (StandardScaler(with_mean=False),), 'ex_preprocs': ()}\n"
     ]
    }
   ],
   "source": [
    "#summarize the best model\n",
    "print(\"Accuracy: %.3f\" % acc)\n",
    "print(model.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:53] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Execution time :  0:14:02.334549\n"
     ]
    }
   ],
   "source": [
    "init_time = datetime.now()\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree',\n",
    "              colsample_bylevel=0.50029110018247, colsample_bynode=1,\n",
    "              colsample_bytree=0.9371080486138555, gamma=0.00014999673883672553,\n",
    "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.15017743323882513, max_delta_step=0, max_depth=10,\n",
    "              min_child_weight=1, missing=None, monotone_constraints='()',\n",
    "              n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n",
    "              objective='multi:softprob', random_state=4,\n",
    "              reg_alpha=6.402887715271464e-05, reg_lambda=1.0516334360090362,\n",
    "              scale_pos_weight=1, seed=4, subsample=0.9995865341563067,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "model.fit(X_train, y_train,verbose=False,\n",
    "            early_stopping_rounds=10,\n",
    "            eval_set=[(X_valid,y_valid)])\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "#print(f\"F-measure of XGB: {mean(scores):.3f}\")\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.57      0.64       587\n",
      "           1       0.77      0.84      0.80      4800\n",
      "           2       0.62      0.58      0.60      2361\n",
      "           3       0.78      0.57      0.66       787\n",
      "           4       0.97      0.98      0.97       845\n",
      "           5       0.95      0.95      0.95      4290\n",
      "           6       0.77      0.69      0.72       846\n",
      "           7       0.90      0.94      0.92      2490\n",
      "           8       0.88      0.88      0.88      1558\n",
      "\n",
      "    accuracy                           0.83     18564\n",
      "   macro avg       0.82      0.78      0.79     18564\n",
      "weighted avg       0.83      0.83      0.82     18564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Find Best Algorithm with Best Params Using TPOT AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "#model = TPOTClassifier(generations=5, population_size=50, cv=cv, scoring='accuracy', \n",
    "                       #verbosity=2, random_state=1, n_jobs=-1)\n",
    "tpot = TPOTClassifier(max_time_mins=120, cv=cv, scoring='accuracy', \n",
    "                       verbosity=2, random_state=1, n_jobs=-1)\n",
    "# perform the search\n",
    "tpot.fit(X_train_scaled, y_train)\n",
    "# export the best model\n",
    "tpot.export('tpot_Otto_best_model.py')\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tpot.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tpot.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "# Average CV score on the training set was: 0.7517570704095573\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=MultinomialNB(alpha=100.0, fit_prior=False)),\n",
    "    DecisionTreeClassifier(criterion=\"entropy\", max_depth=10, min_samples_leaf=12, min_samples_split=7)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "prediction = exported_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Find Best Algorithm with Best Params Using PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pycaret.classification import * \n",
    "from pycaret.datasets import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### PyCaret Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "#fold_strategy = 'kfold' Default = 'stratifiedkfold'\n",
    "clf = setup(data = df_train, target = 'target', session_id=123,\n",
    "            data_split_stratify = True,\n",
    "            normalize = True, \n",
    "            normalize_method = 'minmax',\n",
    "            transformation = True, \n",
    "            transformation_method = 'quantile',\n",
    "            pca = True,\n",
    "            pca_method = 'incremental',\n",
    "            #feature_selection = True,\n",
    "            #feature_selection_threshold = 0.5,\n",
    "            #feature_selection_method = 'boruta',\n",
    "            remove_outliers = True,\n",
    "            outliers_threshold = 0.05,\n",
    "            fix_imbalance = True\n",
    "           )\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Finding optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "#top3_models = compare_models(n_select = 3)\n",
    "best = compare_models()\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "best_model = finalize_model(best)\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "#tuned_model = tune_model(best_model, optimize = 'AUC') # 'Recall'\n",
    "tuned_model = tune_model(best_model, optimize = 'F1')\n",
    "print(tuned_model)\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))\n",
    "print (f\"Job Ended at: {fin_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_model(tuned_model, plot='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_model(tuned_model, 'class_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = predict_model(tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Find Best Params With RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('norm', MinMaxScaler()))\n",
    "estimators.append(('xgb', XGBClassifier(objective='binary:logistic')))\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "                'xgb__n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "                'xgb__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                'xgb__learning_rate': [0.0001, 0.001, 0.01, 0.02, 0.1, 0.2, 0.3], \n",
    "                'xgb__subsample' : [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0],\n",
    "                'xgb__colsample_bytree' : [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0],\n",
    "                'xgb__colsample_bylevel' : [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0],\n",
    "                'xgb__gamma': [0],\n",
    "                'xgb__reg_lambda': [0, 1.0, 3.0, 5.0, 7.0, 10.0, 12.0],\n",
    "                'xgb__scale_pos_weight': [1, 2, 3, 4, 5, 6]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "kfold = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "xgb_grid = RandomizedSearchCV(pipeline,\n",
    "                        parameters,\n",
    "                        cv = kfold,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=False)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "grid_predictions = xgb_grid.predict(X_test)\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    # evaluate model  'accuracy'  'f1_micro'\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    #print('F-measure: %.3f' % score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(base_score=0.5, booster='gbtree',\n",
    "              colsample_bylevel=0.50029110018247, colsample_bynode=1,\n",
    "              colsample_bytree=0.9371080486138555, gamma=0.00014999673883672553,\n",
    "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.15017743323882513, max_delta_step=0, max_depth=10,\n",
    "              min_child_weight=1, missing=None, monotone_constraints='()',\n",
    "              n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n",
    "              objective='multi:softprob', random_state=4,\n",
    "              reg_alpha=6.402887715271464e-05, reg_lambda=1.0516334360090362,\n",
    "              scale_pos_weight=1, seed=4, subsample=0.9995865341563067,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model (X_train, y_train, X_test, y_test):\n",
    "    init_time = datetime.now()\n",
    "    print (f\"Job Started at: {init_time}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Accuracy: %.4f' % (accuracy_score(y_test, predictions)))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    fin_time = datetime.now()\n",
    "    print(\"Execution time : \", (fin_time-init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Started at: 2020-11-20 10:52:35.977893\n",
      "[10:52:36] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Accuracy: 0.8283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.58      0.65       587\n",
      "           1       0.77      0.84      0.80      4800\n",
      "           2       0.62      0.58      0.60      2361\n",
      "           3       0.78      0.57      0.66       787\n",
      "           4       0.97      0.98      0.97       845\n",
      "           5       0.95      0.95      0.95      4290\n",
      "           6       0.76      0.68      0.72       846\n",
      "           7       0.90      0.94      0.92      2490\n",
      "           8       0.88      0.88      0.88      1558\n",
      "\n",
      "    accuracy                           0.83     18564\n",
      "   macro avg       0.82      0.78      0.79     18564\n",
      "weighted avg       0.83      0.83      0.83     18564\n",
      "\n",
      "Execution time :  0:11:05.795835\n"
     ]
    }
   ],
   "source": [
    "fit_model (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interpretation with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = datetime.now()\n",
    "print (f\"Job Started at: {init_time}\")\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "fin_time = datetime.now()\n",
    "print(\"Execution time : \", (fin_time-init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[3, :], X.iloc[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('feat_90', shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Normality Test\n",
    "\n",
    "* NN Rule: Normalize Non-Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Standardize Only Gaussian-Like Input Variables\n",
    "* Normalize Only Non-Gaussian Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_normal(data):\n",
    "    alpha = 0.05\n",
    "    stat, p = normaltest(data)\n",
    "    if p > alpha:\n",
    "        normalTest = True\n",
    "    else:\n",
    "        normalTest = False\n",
    "    \n",
    "    stat, p = shapiro(data)\n",
    "    if p > alpha:\n",
    "        shapiroTest = True\n",
    "    else:\n",
    "        shapiroTest = False\n",
    "        \n",
    "    return normalTest and shapiroTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Like columns: []\n",
      "Non-Gaussian Like columns: ['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'feat_64', 'feat_65', 'feat_66', 'feat_67', 'feat_68', 'feat_69', 'feat_70', 'feat_71', 'feat_72', 'feat_73', 'feat_74', 'feat_75', 'feat_76', 'feat_77', 'feat_78', 'feat_79', 'feat_80', 'feat_81', 'feat_82', 'feat_83', 'feat_84', 'feat_85', 'feat_86', 'feat_87', 'feat_88', 'feat_89', 'feat_90', 'feat_91', 'feat_92', 'feat_93']\n"
     ]
    }
   ],
   "source": [
    "Gaussian_Like = []\n",
    "Non_Gaussian = []\n",
    "\n",
    "for i, name in enumerate (X.columns):\n",
    "    if is_normal(X[name]):\n",
    "        Gaussian_Like.append(name)\n",
    "    else:\n",
    "        Non_Gaussian.append(name)\n",
    "        \n",
    "print (f\"Gaussian Like columns: {Gaussian_Like}\")\n",
    "print (f\"Non-Gaussian Like columns: {Non_Gaussian}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Standardize Only Gaussian-Like Input Variables\n",
    "# Normalize Only Non-Gaussian Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if len(Gaussian_Like)> 0 and len(Non_Gaussian) > 0:\n",
    "    print(\"Normality Function is working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Remove correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Identify Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
      "feat_1   1.000000  0.031332  0.027807  0.027529  0.042973  0.043603  0.298952   \n",
      "feat_2   0.031332  1.000000  0.082573  0.134987  0.020926  0.041343  0.222386   \n",
      "feat_3   0.027807  0.082573  1.000000  0.583523  0.010880  0.004288  0.001294   \n",
      "feat_4   0.027529  0.134987  0.583523  1.000000  0.017290  0.014059  0.014490   \n",
      "feat_5   0.042973  0.020926  0.010880  0.017290  1.000000  0.145355  0.075047   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "feat_89  0.096851  0.105527  0.174781  0.183715  0.119951  0.035042  0.063511   \n",
      "feat_90  0.010310  0.515022  0.015068  0.009454  0.004842  0.054034  0.129578   \n",
      "feat_91  0.037264  0.026383  0.012417  0.010312  0.012012  0.012465  0.068506   \n",
      "feat_92  0.054777  0.008219  0.066921  0.087631  0.065331  0.015479  0.032261   \n",
      "feat_93  0.081783  0.054593  0.006814  0.015746  0.002038  0.008521  0.034912   \n",
      "\n",
      "           feat_8    feat_9   feat_10  ...   feat_84   feat_85   feat_86  \\\n",
      "feat_1   0.056321  0.032285  0.097776  ...  0.049634  0.008739  0.107947   \n",
      "feat_2   0.019815  0.025630  0.051925  ...  0.009845  0.006764  0.039090   \n",
      "feat_3   0.053462  0.063551  0.036944  ...  0.011159  0.048626  0.096093   \n",
      "feat_4   0.046184  0.046250  0.059514  ...  0.005684  0.033153  0.071029   \n",
      "feat_5   0.035861  0.024708  0.091324  ...  0.467329  0.034062  0.013879   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "feat_89  0.007974  0.019147  0.061498  ...  0.103643  0.053582  0.011822   \n",
      "feat_90  0.026807  0.020698  0.049908  ...  0.006013  0.003931  0.019803   \n",
      "feat_91  0.095990  0.014742  0.024025  ...  0.003444  0.023091  0.024005   \n",
      "feat_92  0.013608  0.069707  0.006869  ...  0.048431  0.043484  0.049393   \n",
      "feat_93  0.005131  0.006038  0.041316  ...  0.003723  0.023390  0.029035   \n",
      "\n",
      "          feat_87   feat_88   feat_89   feat_90   feat_91   feat_92   feat_93  \n",
      "feat_1   0.089374  0.020830  0.096851  0.010310  0.037264  0.054777  0.081783  \n",
      "feat_2   0.047451  0.047035  0.105527  0.515022  0.026383  0.008219  0.054593  \n",
      "feat_3   0.009838  0.082336  0.174781  0.015068  0.012417  0.066921  0.006814  \n",
      "feat_4   0.005055  0.067484  0.183715  0.009454  0.010312  0.087631  0.015746  \n",
      "feat_5   0.013999  0.019201  0.119951  0.004842  0.012012  0.065331  0.002038  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "feat_89  0.066008  0.022552  1.000000  0.027764  0.015917  0.129622  0.030650  \n",
      "feat_90  0.014696  0.031679  0.027764  1.000000  0.014812  0.035311  0.039864  \n",
      "feat_91  0.028850  0.033653  0.015917  0.014812  1.000000  0.104226  0.000045  \n",
      "feat_92  0.001424  0.070120  0.129622  0.035311  0.104226  1.000000  0.003653  \n",
      "feat_93  0.499990  0.008631  0.030650  0.039864  0.000045  0.003653  1.000000  \n",
      "\n",
      "[93 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "print(); print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
      "feat_1      NaN  0.031332  0.027807  0.027529  0.042973  0.043603  0.298952   \n",
      "feat_2      NaN       NaN  0.082573  0.134987  0.020926  0.041343  0.222386   \n",
      "feat_3      NaN       NaN       NaN  0.583523  0.010880  0.004288  0.001294   \n",
      "feat_4      NaN       NaN       NaN       NaN  0.017290  0.014059  0.014490   \n",
      "feat_5      NaN       NaN       NaN       NaN       NaN  0.145355  0.075047   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "feat_89     NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "feat_90     NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "feat_91     NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "feat_92     NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "feat_93     NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "           feat_8    feat_9   feat_10  ...   feat_84   feat_85   feat_86  \\\n",
      "feat_1   0.056321  0.032285  0.097776  ...  0.049634  0.008739  0.107947   \n",
      "feat_2   0.019815  0.025630  0.051925  ...  0.009845  0.006764  0.039090   \n",
      "feat_3   0.053462  0.063551  0.036944  ...  0.011159  0.048626  0.096093   \n",
      "feat_4   0.046184  0.046250  0.059514  ...  0.005684  0.033153  0.071029   \n",
      "feat_5   0.035861  0.024708  0.091324  ...  0.467329  0.034062  0.013879   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "feat_89       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
      "feat_90       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
      "feat_91       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
      "feat_92       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
      "feat_93       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
      "\n",
      "          feat_87   feat_88   feat_89   feat_90   feat_91   feat_92   feat_93  \n",
      "feat_1   0.089374  0.020830  0.096851  0.010310  0.037264  0.054777  0.081783  \n",
      "feat_2   0.047451  0.047035  0.105527  0.515022  0.026383  0.008219  0.054593  \n",
      "feat_3   0.009838  0.082336  0.174781  0.015068  0.012417  0.066921  0.006814  \n",
      "feat_4   0.005055  0.067484  0.183715  0.009454  0.010312  0.087631  0.015746  \n",
      "feat_5   0.013999  0.019201  0.119951  0.004842  0.012012  0.065331  0.002038  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "feat_89       NaN       NaN       NaN  0.027764  0.015917  0.129622  0.030650  \n",
      "feat_90       NaN       NaN       NaN       NaN  0.014812  0.035311  0.039864  \n",
      "feat_91       NaN       NaN       NaN       NaN       NaN  0.104226  0.000045  \n",
      "feat_92       NaN       NaN       NaN       NaN       NaN       NaN  0.003653  \n",
      "feat_93       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[93 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "print(); print(upper_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feat_45']\n"
     ]
    }
   ],
   "source": [
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Drop Marked Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
      "0       1       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       1       0   \n",
      "2       0       0       0       0       0       0       0       1       0   \n",
      "3       1       0       0       1       6       1       5       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   feat_10  ...  feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  \\\n",
      "0        0  ...        0        1        0        0        0        0   \n",
      "1        0  ...        0        0        0        0        0        0   \n",
      "2        0  ...        0        0        0        0        0        0   \n",
      "3        1  ...       22        0        1        2        0        0   \n",
      "4        0  ...        0        1        0        0        0        0   \n",
      "\n",
      "   feat_90  feat_91  feat_92  feat_93  \n",
      "0        0        0        0        0  \n",
      "1        0        0        0        0  \n",
      "2        0        0        0        0  \n",
      "3        0        0        0        0  \n",
      "4        1        0        0        0  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop features \n",
    "if len(to_drop) > 0:\n",
    "    X = X.drop(X[to_drop], axis=1)\n",
    "    print (X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Started at: 2020-11-13 23:49:47.349071\n",
      "[23:49:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.57      0.65       587\n",
      "           1       0.76      0.84      0.80      4800\n",
      "           2       0.62      0.57      0.60      2361\n",
      "           3       0.78      0.57      0.66       787\n",
      "           4       0.97      0.98      0.98       845\n",
      "           5       0.95      0.95      0.95      4290\n",
      "           6       0.77      0.69      0.73       846\n",
      "           7       0.90      0.94      0.92      2490\n",
      "           8       0.88      0.88      0.88      1558\n",
      "\n",
      "    accuracy                           0.83     18564\n",
      "   macro avg       0.82      0.78      0.79     18564\n",
      "weighted avg       0.82      0.83      0.82     18564\n",
      "\n",
      "Execution time :  0:15:22.449911\n"
     ]
    }
   ],
   "source": [
    "fit_model (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Combined Feature Selection Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "    # define the base models\n",
    "    models, names = list(), list()\n",
    "    # anova\n",
    "    fs = SelectKBest(score_func=f_classif, k=n_features)\n",
    "    anova = Pipeline([('fs', fs), ('m', model)])\n",
    "    models.append(('anova', anova))\n",
    "    names.append('anova')\n",
    "    # mutual information\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "    mutinfo = Pipeline([('fs', fs), ('m', model)])\n",
    "    models.append(('mutinfo', mutinfo))\n",
    "    names.append('mutinfo')\n",
    "    # rfe\n",
    "    fs = RFE(estimator=RandomForestClassifier(), n_features_to_select=n_features)\n",
    "    rfe = Pipeline([('fs', fs), ('m', model)])\n",
    "    models.append(('rfe', rfe))\n",
    "    names.append('rfe')\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    names.append('ensemble')\n",
    "    return names, [anova, mutinfo, rfe, ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-47b38cf37d8c>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-47b38cf37d8c>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print('>%s: %.3f (%.3f)' % (name, mean(n_scores), std(n_scores)))results.append(n_scores)\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# get the ensemble model\n",
    "names, models = get_ensemble(k)\n",
    "# evaluate each model\n",
    "results = list()\n",
    "for model,name in zip(models,names):\n",
    "    # define the evaluation method\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # evaluate the model on the dataset\n",
    "    n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    print('>%s: %.3f (%.3f)' % (name, mean(n_scores), std(n_scores)))\n",
    "    results.append(n_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0.8281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.58      0.65       587\n",
      "           1       0.77      0.84      0.80      4800\n",
      "           2       0.62      0.58      0.60      2361\n",
      "           3       0.78      0.57      0.66       787\n",
      "           4       0.97      0.98      0.97       845\n",
      "           5       0.95      0.95      0.95      4290\n",
      "           6       0.77      0.68      0.72       846\n",
      "           7       0.90      0.94      0.92      2490\n",
      "           8       0.88      0.88      0.88      1558\n",
      "\n",
      "    accuracy                           0.83     18564\n",
      "   macro avg       0.82      0.78      0.79     18564\n",
      "weighted avg       0.83      0.83      0.83     18564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "print('> %.4f' % (accuracy_score(y_test, predictions)))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Convert or export the model into ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import convert_sklearn, to_onnx, update_registered_converter\n",
    "from skl2onnx.common.shape_calculator import (\n",
    "    calculate_linear_classifier_output_shapes,\n",
    "    calculate_linear_regressor_output_shapes)\n",
    "from onnxmltools.convert.xgboost.operator_converters.XGBoost import (\n",
    "    convert_xgboost)\n",
    "from onnxmltools.convert import convert_xgboost as convert_xgboost_booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#For XGBoost only\n",
    "update_registered_converter(\n",
    "    XGBClassifier, 'XGBoostXGBClassifier',\n",
    "    calculate_linear_classifier_output_shapes, convert_xgboost,\n",
    "    options={'nocl': [True, False], 'zipmap': [True, False]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "MissingShapeCalculator",
     "evalue": "Unable to find a shape calculator for type '<class 'sklearn.preprocessing._data.QuantileTransformer'>'.\nIt usually means the pipeline being converted contains a\ntransformer or a predictor with no corresponding converter\nimplemented in sklearn-onnx. If the converted is implemented\nin another library, you need to register\nthe converted so that it can be used by sklearn-onnx (function\nupdate_registered_converter). If the model is not yet covered\nby sklearn-onnx, you may raise an issue to\nhttps://github.com/onnx/sklearn-onnx/issues\nto get the converter implemented or even contribute to the\nproject. If the model is a custom model, a new converter must\nbe implemented. Examples can be found in the gallery.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingShapeCalculator\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-00fc2521868b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_onnx = convert_sklearn(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xgboost_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatTensorType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     target_opset=12)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/convert.py\u001b[0m in \u001b[0;36mconvert_sklearn\u001b[0;34m(model, name, initial_types, doc_string, target_opset, custom_conversion_functions, custom_shape_calculators, custom_parsers, options, dtype, intermediate, white_op, black_op, final_types)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Infer variable shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Convert our Topology object into ONNX. The outcome is an ONNX model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_all_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36m_infer_all_types\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m                 \u001b[0mshape_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resolve_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36minfer_types\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# Invoke a core inference function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             raise MissingShapeCalculator(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \"Unable to find a shape calculator for type '{}'.\".format(\n\u001b[1;32m    215\u001b[0m                     type(self.raw_operator)))\n",
      "\u001b[0;31mMissingShapeCalculator\u001b[0m: Unable to find a shape calculator for type '<class 'sklearn.preprocessing._data.QuantileTransformer'>'.\nIt usually means the pipeline being converted contains a\ntransformer or a predictor with no corresponding converter\nimplemented in sklearn-onnx. If the converted is implemented\nin another library, you need to register\nthe converted so that it can be used by sklearn-onnx (function\nupdate_registered_converter). If the model is not yet covered\nby sklearn-onnx, you may raise an issue to\nhttps://github.com/onnx/sklearn-onnx/issues\nto get the converter implemented or even contribute to the\nproject. If the model is a custom model, a new converter must\nbe implemented. Examples can be found in the gallery.\n"
     ]
    }
   ],
   "source": [
    "model_onnx = convert_sklearn(\n",
    "    pipeline, 'xgboost_model',\n",
    "    [('input', FloatTensorType([None, 2]))],\n",
    "    target_opset=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# And save.\n",
    "with open(\"pipeline_xgboost.onnx\", \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"predict\", pipeline.predict(X_test[:5]))\n",
    "print(\"predict_proba\", pipeline.predict_proba(X_test[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load and run the model using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"pipeline_xgboost.onnx\")\n",
    "pred_onx = sess.run(None, {\"input\": X_test[:5].astype(numpy.float32)})\n",
    "print(\"predict\", pred_onx[0])\n",
    "print(\"predict_proba\", pred_onx[1][:1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
