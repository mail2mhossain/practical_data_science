{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
      "Total Tokens: 118684\n",
      "Unique Tokens: 7409\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 118633\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size, seq_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
      "[1046, 11, 11, 1045, 329, 7409, 4, 1, 2873, 35, 213, 1, 261, 3, 2251, 9, 11, 179, 817, 123, 92, 2872, 4, 1, 2249, 7408, 1, 7407, 7406, 2, 75, 120, 11, 1266, 4, 110, 6, 30, 168, 16, 49, 7405, 1, 1609, 13, 57, 8, 549, 151, 11, 57]\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7410\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
       "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
       "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
       "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
       "         13,   57,    8,  549,  151,   11])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118633, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "927/927 [==============================] - 189s 204ms/step - loss: 6.1386 - accuracy: 0.0744\n",
      "Epoch 2/100\n",
      "927/927 [==============================] - 199s 215ms/step - loss: 5.6793 - accuracy: 0.1066\n",
      "Epoch 3/100\n",
      "927/927 [==============================] - 173s 187ms/step - loss: 5.4510 - accuracy: 0.1304\n",
      "Epoch 4/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 5.2938 - accuracy: 0.1440\n",
      "Epoch 5/100\n",
      "927/927 [==============================] - 195s 210ms/step - loss: 5.1797 - accuracy: 0.1520\n",
      "Epoch 6/100\n",
      "927/927 [==============================] - 197s 213ms/step - loss: 5.0864 - accuracy: 0.1587\n",
      "Epoch 7/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 5.0069 - accuracy: 0.1637\n",
      "Epoch 8/100\n",
      "927/927 [==============================] - 188s 202ms/step - loss: 4.9346 - accuracy: 0.1674\n",
      "Epoch 9/100\n",
      "927/927 [==============================] - 184s 199ms/step - loss: 4.8652 - accuracy: 0.1712\n",
      "Epoch 10/100\n",
      "927/927 [==============================] - 184s 198ms/step - loss: 4.8002 - accuracy: 0.1744\n",
      "Epoch 11/100\n",
      "927/927 [==============================] - 184s 198ms/step - loss: 4.7360 - accuracy: 0.1780\n",
      "Epoch 12/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 4.6725 - accuracy: 0.1808\n",
      "Epoch 13/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 4.6131 - accuracy: 0.1835\n",
      "Epoch 14/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 4.5559 - accuracy: 0.1859\n",
      "Epoch 15/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 4.4995 - accuracy: 0.1897\n",
      "Epoch 16/100\n",
      "927/927 [==============================] - 185s 199ms/step - loss: 4.4470 - accuracy: 0.1917\n",
      "Epoch 17/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 4.3955 - accuracy: 0.1940\n",
      "Epoch 18/100\n",
      "927/927 [==============================] - 189s 204ms/step - loss: 4.3463 - accuracy: 0.1962\n",
      "Epoch 19/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 4.3019 - accuracy: 0.1994\n",
      "Epoch 20/100\n",
      "927/927 [==============================] - 184s 198ms/step - loss: 4.2573 - accuracy: 0.2017\n",
      "Epoch 21/100\n",
      "927/927 [==============================] - 189s 204ms/step - loss: 4.2181 - accuracy: 0.2042\n",
      "Epoch 22/100\n",
      "927/927 [==============================] - 190s 204ms/step - loss: 4.1781 - accuracy: 0.2065\n",
      "Epoch 23/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 4.1414 - accuracy: 0.2092\n",
      "Epoch 24/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 4.1063 - accuracy: 0.2117\n",
      "Epoch 25/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 4.0739 - accuracy: 0.2149\n",
      "Epoch 26/100\n",
      "927/927 [==============================] - 185s 199ms/step - loss: 4.0409 - accuracy: 0.2172\n",
      "Epoch 27/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 4.0111 - accuracy: 0.2200\n",
      "Epoch 28/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 3.9821 - accuracy: 0.2216\n",
      "Epoch 29/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.9544 - accuracy: 0.2252\n",
      "Epoch 30/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.9276 - accuracy: 0.2272\n",
      "Epoch 31/100\n",
      "927/927 [==============================] - 194s 209ms/step - loss: 3.9006 - accuracy: 0.2307\n",
      "Epoch 32/100\n",
      "927/927 [==============================] - 189s 204ms/step - loss: 3.8757 - accuracy: 0.2328\n",
      "Epoch 33/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 3.8511 - accuracy: 0.2354\n",
      "Epoch 34/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.8259 - accuracy: 0.2375\n",
      "Epoch 35/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 3.8040 - accuracy: 0.2397\n",
      "Epoch 36/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.7806 - accuracy: 0.2434\n",
      "Epoch 37/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.7592 - accuracy: 0.2442\n",
      "Epoch 38/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.7342 - accuracy: 0.2493\n",
      "Epoch 39/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.7139 - accuracy: 0.2508\n",
      "Epoch 40/100\n",
      "927/927 [==============================] - 185s 199ms/step - loss: 3.6910 - accuracy: 0.2530\n",
      "Epoch 41/100\n",
      "927/927 [==============================] - 190s 205ms/step - loss: 3.6712 - accuracy: 0.2555\n",
      "Epoch 42/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.6499 - accuracy: 0.2573\n",
      "Epoch 43/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.6282 - accuracy: 0.2608\n",
      "Epoch 44/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 3.6065 - accuracy: 0.2634\n",
      "Epoch 45/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.5893 - accuracy: 0.2649\n",
      "Epoch 46/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 3.5695 - accuracy: 0.2676\n",
      "Epoch 47/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.5506 - accuracy: 0.2690\n",
      "Epoch 48/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.5314 - accuracy: 0.2716\n",
      "Epoch 49/100\n",
      "927/927 [==============================] - 184s 199ms/step - loss: 3.5114 - accuracy: 0.2757\n",
      "Epoch 50/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 3.4936 - accuracy: 0.2776\n",
      "Epoch 51/100\n",
      "927/927 [==============================] - 188s 202ms/step - loss: 3.4757 - accuracy: 0.2792\n",
      "Epoch 52/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 3.4568 - accuracy: 0.2814\n",
      "Epoch 53/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.4380 - accuracy: 0.2844\n",
      "Epoch 54/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.4182 - accuracy: 0.2869\n",
      "Epoch 55/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.4034 - accuracy: 0.2895\n",
      "Epoch 56/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.3862 - accuracy: 0.2924\n",
      "Epoch 57/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 3.3683 - accuracy: 0.2932\n",
      "Epoch 58/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.3516 - accuracy: 0.2965\n",
      "Epoch 59/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 3.3377 - accuracy: 0.2992\n",
      "Epoch 60/100\n",
      "927/927 [==============================] - 190s 205ms/step - loss: 3.3193 - accuracy: 0.3008\n",
      "Epoch 61/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.3027 - accuracy: 0.3038\n",
      "Epoch 62/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.2863 - accuracy: 0.3058\n",
      "Epoch 63/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.2706 - accuracy: 0.3087\n",
      "Epoch 64/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.2561 - accuracy: 0.3110\n",
      "Epoch 65/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.2397 - accuracy: 0.3129\n",
      "Epoch 66/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.2250 - accuracy: 0.3152\n",
      "Epoch 67/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 3.2091 - accuracy: 0.3180\n",
      "Epoch 68/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.1930 - accuracy: 0.3200\n",
      "Epoch 69/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.1800 - accuracy: 0.3218\n",
      "Epoch 70/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.1638 - accuracy: 0.3253\n",
      "Epoch 71/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.1485 - accuracy: 0.3270\n",
      "Epoch 72/100\n",
      "927/927 [==============================] - 189s 203ms/step - loss: 3.1343 - accuracy: 0.3295\n",
      "Epoch 73/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 3.1205 - accuracy: 0.3312\n",
      "Epoch 74/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.1061 - accuracy: 0.3337\n",
      "Epoch 75/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 3.0903 - accuracy: 0.3365\n",
      "Epoch 76/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 3.0755 - accuracy: 0.3392\n",
      "Epoch 77/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 3.0629 - accuracy: 0.3399\n",
      "Epoch 78/100\n",
      "927/927 [==============================] - 185s 200ms/step - loss: 3.0496 - accuracy: 0.3433\n",
      "Epoch 79/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 3.0349 - accuracy: 0.3459\n",
      "Epoch 80/100\n",
      "927/927 [==============================] - 189s 204ms/step - loss: 3.0205 - accuracy: 0.3480\n",
      "Epoch 81/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 3.0074 - accuracy: 0.3485\n",
      "Epoch 82/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 2.9960 - accuracy: 0.3510\n",
      "Epoch 83/100\n",
      "927/927 [==============================] - 185s 199ms/step - loss: 2.9797 - accuracy: 0.3536\n",
      "Epoch 84/100\n",
      "927/927 [==============================] - 186s 200ms/step - loss: 2.9682 - accuracy: 0.3547\n",
      "Epoch 85/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 2.9545 - accuracy: 0.3592\n",
      "Epoch 86/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 2.9399 - accuracy: 0.3614\n",
      "Epoch 87/100\n",
      "927/927 [==============================] - 199s 214ms/step - loss: 2.9281 - accuracy: 0.3627\n",
      "Epoch 88/100\n",
      "927/927 [==============================] - 202s 218ms/step - loss: 2.9132 - accuracy: 0.3660\n",
      "Epoch 89/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 2.9033 - accuracy: 0.3664\n",
      "Epoch 90/100\n",
      "927/927 [==============================] - 188s 203ms/step - loss: 2.8891 - accuracy: 0.3697\n",
      "Epoch 91/100\n",
      "927/927 [==============================] - 188s 202ms/step - loss: 2.8772 - accuracy: 0.3714\n",
      "Epoch 92/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 2.8645 - accuracy: 0.3740\n",
      "Epoch 93/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 2.8500 - accuracy: 0.3762\n",
      "Epoch 94/100\n",
      "927/927 [==============================] - 187s 201ms/step - loss: 2.8385 - accuracy: 0.3779\n",
      "Epoch 95/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 2.8286 - accuracy: 0.3796\n",
      "Epoch 96/100\n",
      "927/927 [==============================] - 187s 202ms/step - loss: 2.8151 - accuracy: 0.3823\n",
      "Epoch 97/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 2.8038 - accuracy: 0.3844\n",
      "Epoch 98/100\n",
      "927/927 [==============================] - 186s 201ms/step - loss: 2.7889 - accuracy: 0.3871\n",
      "Epoch 99/100\n",
      "927/927 [==============================] - 192s 207ms/step - loss: 2.7796 - accuracy: 0.3884\n",
      "Epoch 100/100\n",
      "927/927 [==============================] - 192s 207ms/step - loss: 2.7655 - accuracy: 0.3902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6968050b50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('nl_Text_genertion_model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('nl_Text_genertion_tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('nl_Text_genertion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open('nl_Text_genertion_tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carried on by them how why of course they will go on expeditions together and will take with them any of their children who are strong enough that after the manner of the artisans child they may look on at the work which they will have to do when they are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-ae4130c5565f>:12: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "grown up at the same time highspirited and magnanimous are not to be ascertained the shore i said to the state of epeus the soul of aeschylus will be the best of the soul scaring the other to the hatred of his which we call the shafts of the soul\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4CV",
   "language": "python",
   "name": "dl4cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
